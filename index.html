<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A visuotactile system capable of folding and hanging in-air using dense visual representations and tactilely-supervised visual affordance networks.">
  <meta name="keywords" content="Deformable Object Manipulation, Dense Correspondence Learning, Confidence-Aware Planning, Visuotactile Perception">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/shirticon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
<!--
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>
-->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="http://nehasunil.com/" target="_blank">Neha Sunil</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://mhtippur.github.io/web/" target="_blank">Megha Tippur</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a> Arnau Portillo</a>,
                    <span class="author-block">
                    <a href="https://persci.mit.edu/people/adelson/" target="_blank"> Edward Adelson</a>,
                    <span class="author-block">
                    <a href="https://mcube.mit.edu/index.html" target="_blank"> Alberto Rodriguez</a>
                  </span>
          </div>

                <div class="is-size-5 publication-authors">
                  <img src="static/images/mit_logo_red.png" alt="MIT Logo" style="height: 2.5em; vertical-align: middle;"><br>
                  CoRL 2025 <i>(Oral Presentation)</i></span>
                  <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="static/pdfs/CoRL2025_InAirReactiveManipulation.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Supplementary Link-->
              <span class="link-block">
                <a href="static/pdfs/CoRL2025_Supplemental_InAirReactiveManipulation.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>
              <span class="link-block">
                <a href="http://arxiv.org/abs/2509.03889"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. 
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. 
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. 
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser_video.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-left">
        <i><strong>Overview:</strong> We develop a visuotactile system capable of folding and hanging in-air using 
        dense visual representations and tactilely-supervised visual affordance networks.</i>
      </h2>
    </div>
  </div>
</section>

<section class="section pt-3">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div style="background-color: #f5f5f5; padding: 1.5rem; border-radius: 6px;">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Manipulating clothing is challenging due to complex configurations, variable material dynamics, and frequent self-occlusion. 
          Prior systems often flatten garments or assume visibility of key features.
          We present a dual-arm visuotactile framework that combines confidence-aware dense visual correspondence and tactile-supervised grasp affordance to operate directly on crumpled and suspended garments. 
          </p>
          <p>
          The correspondence model is trained on a custom, high-fidelity simulated dataset using a distributional loss that captures cloth symmetries and generates correspondence confidence estimates. 
          These estimates guide a reactive state machine that adapts folding strategies based on perceptual uncertainty. 
          In parallel, a visuotactile grasp affordance network, self-supervised using high-resolution tactile feedback, determines which regions are physically graspable. 
          The same tactile classifier is used during execution for real-time grasp validation. 
          By deferring action in low-confidence states, the system handles highly occluded table-top and in-air configurations. 
          We demonstrate our task-agnostic grasp selection module in folding and hanging tasks. 
          Moreover, our dense descriptors provide a reusable intermediate representation for other planning modalities, such as extracting grasp targets from human video demonstrations, paving the way for more generalizable and scalable garment manipulation.
          </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <div style="margin-top: 2rem;"></div>

    <!-- Simulation -->
     <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Dataset Generation in Simulation</h2>
        <div class="content has-text-justified">
          <!-- <p>
            We use <strong>Blender</strong> to simulate a wide variety of shirt geometries and deformations, generating a large RGB-D dataset (1500 scenes) for training. 
            In addition to parameterizing the overall geometries, we incorporate <strong>hems, stitches, and sewing seams</strong> into our shirts to mimic realistic garments, enhancing visual realism 
            and providing key features helpful for correspondence. 
            Our method incorporates these finer details while <strong>preserving consistent vertex indexing across shirts</strong>, 
            enabling descriptors to align with a canonical template regardless of geometry, without relying on sparse skeleton keypoints.
          </p> -->
          <p>
            We use <strong>Blender</strong> to simulate a wide variety of shirt geometries and deformations. We incorporate <strong>hems, stitches, and sewing seams </strong> 
            into our simulations to mimic realistic garments, ensuring visual realism and providing key features for correspondence. We use <strong>consistent vertex 
              indexing across shirts </strong>, enabling descriptors to align with a canonical template. 
          </p>
          
          <!-- Simulation Image -->
      <div class="simulation-image">
        <img src="static/images/blender_simulation_methodology.png" alt="Shirt simulation example"
            style="max-width:100%; height:auto; margin-top:1.5rem; border-radius:8px; margin-bottom: 1.5rem;">

          <p>
            Our animated simulation pipeline enables generation of a wide range of shirt types and configurations. Shown below are a few examples 
            of different simulated shirts in both suspended in-air and on-table configurations. 
          </p>
          
          <div class="example-shirts-image">
            <img src="static/images/example_simulated_shirts.png" alt="Shirt simulation example"
                style="max-width:100%; height:auto; margin-top:1.5rem; border-radius:8px;">
                <p style="font-size:1em; color:#000000; text-align:center"><i>Example shirts simulated in Blender.</i>
        </div>
      </div>
    </div>
    <!--/ Simulation -->

    <div style="margin-top: 5rem;"></div>
    <!--Dense Correspondence with Distributive Loss -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Dense Correspondence with Distributional Loss</h2>
        <div class="content has-text-justified">

          <!-- Network Training Architecture -->

          <p>
            To train the dense correspondence network, a pixel on the deformed shirt is queried. Because the vertex scheme across the different simulated shirts is consistent with 
            the canonical shirt mapping, the corresponding pixel match in simulation is found. The network outputs a dense descriptor for the shirt 
            where every pixel is mapped to an d-dimensional embedding, capturing the location of the pixel with respect to the canonical shirt in the descriptor space. 
          </p>
          <div class="training-image">
            <img src="static/images/training_new.png" alt="Shirt simulation example"
                style="max-width:100%; height:auto; margin-top:1.5rem; border-radius:8px;">
          </div>
          <div style="margin-top: 2rem;"></div>

          <!-- <p>
            <strong>MHT MAYBE TAKE THIS PARAGRAPH OUT - TOO IN DEPTH FOR WEBSITE! </strong>Additionally, we train the network using a distributional loss. A multimodal isotropic Gaussian with modes at symmetric match points is used as 
            the target distribution. The probability estimator is found by taking the softmax over negative distances in the descriptor space. The model is 
            trained by minimizing the difference between the target distribution and the probability estimate, as measured by KL divergence. Training with 
            a distributional loss is advantageous because 
          </p> -->
          <p>
            Using a <strong>Distributional Loss </strong> for training our dense correspondence network provides several advantages, including allowing the network 
            to represent multiple valid matches for a pixel, <strong>capturing symmetries in the cloth</strong>, and by providing a <strong>built-in confidence metric</strong> needed to manipulate 
            garments in highly occluded states.  
          </p>
          <p>
            The resulting network can query points on deformed shirts and get correspondence heatmaps with respect to the canonical shirt. 
            We can also use our descriptors in the <strong>inverse direction</strong> to query points on the canonical shirt and find correspondences on the deformed shirt. 
            <strong>This is the direction we query in during execution on the robot.</strong>
          </p>
          <div class="training-image" style="margin-top: 0rem; margin-bottom: 0rem;">
            <img src="static/images/dense_correspondence.png" alt="Shirt simulation example"
                style="max-width:100%; height:auto; margin-top:1.5rem; border-radius:8px;">
          </div>
        

      </div>
    </div>
    <!--/ Dense Correspondence with Distributive Loss -->
  </div>

  <div style="margin-top: 2rem;"></div>
    <!--Visuotactile Grasp Affordance -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Visuotactile Grasp Affordance Network</h2>
        <div class="content has-text-justified">
          <p>
          We train a <strong>grasp affordance network to find good grasp regions of a hanging shirt </strong> using the simulated dataset. A depth image of the garment 
          is input into the UNet archiecture implemented in <a href="https://arxiv.org/abs/2212.05108">[1]</a> and a heatmap representing the graspable regions is output. 
          </p>
          <p>By training on the simulated shirt dataset, we can explicitly check desired grasping criteria, such as <strong> (1) side grasp reachability, 
            (2) gripper collision, and (3) whether or not two layers of fabric or less are being grasped. </strong>
          </strong></p>
          <div class="training-image">
            <img src="static/images/affordance_training_in_simulation.png" alt="Shirt simulation example"
                style="max-width:100%; height:auto; margin-top:1.5rem; border-radius:8px; ">
              <p style="font-size:1em; color:#000000; text-align:center"><i>Visuotactile grasp affordance training in simulation.</i>
          </div>
          <div style="margin-top: 1rem;"></div>
          <p> However, because there are a wide range of material properties and dynamics from varied fabrics that cannot be fully captured in simulation, 
            we <strong>fine-tune on the robot using a tactile classifier to supervise grasp success</strong>. The visualized grasp affordance 
            shows predictions from the model trained in simulation, and the selected grasps based on that affordance to improve efficiency of grasp selection.
          </p>
          <div style="margin-top: 3rem;">
            <video id="affordance_finetuning" autoplay muted loop playsinline controls height="100%"> 
              <source src="./static/videos/finetuning_grasp_affordance_website.mp4"
                type="video/mp4">
            </video>
             <p style="font-size:1em; color:#000000; text-align: center;"><i>We repeat this fine-tuning processing for a range of shirt types to capture a range of material properties
              and dynamics.</i>
             </p>
             </div>
      <h2 class="subtitle has-text-left">
      </div>
    </div>
    </div>
    <!--/ Visuotactile Grasp Affordance -->

    <div style="margin-top: 3rem;"></div>
    <!--In-Air Garment Manipulation-->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">In Air-Garment Manipulation</h2>
        <div class="content has-text-justified">
            <p>By <strong>combining confidence-aware dense visual correspondence and a visuotactile grasp affordance network</strong>, 
            our system can <strong>defer action in low confidence states</strong>, 
            allowing us to handle highly occluded table-top and in-air configurations. </p>

            <p>
            In the single-grasp example below, a point on the end of the sleeve is queried on the canonical shirt. 
            The system starts with a relatively bad affordance for the sleeve since the surface normal is not aligned with the gripper. 
            As the shirt rotates, the sleeve moves to a better position for grasping, and the network becomes more confident that the point is indeed a sleeve. 
            Once a high correspondence point has a good affordance, the robot attempts to grasp the desired point. 
            </p>
          
          <div style="margin-top: 1rem; margin-bottom: 3rem;">
            <video id="affordance_finetuning" autoplay muted loop playsinline controls height="100%"> 
              <source src="./static/videos/single_affordance_grasp_video_web.mp4"
                type="video/mp4">
            </video>
             <p style="font-size:1em; color:#000000;"><i>Example of robot deferring grasp action until the queried point (end of the sleeve) has a high enough correspondence match 
              and visuotactile grasp affordance. </i></p>

          </div>

          <p>We evaluate grasping performance across four garment categories <strong>(sleeve, bottom, shoulder, and collar)</strong> using two different correspondence networks: 
            one trained solely on suspended shirts and another on a combined table and  suspended dataset. For each category, we perform <strong>10 grasp attempts</strong> per network, 
            recording outcomes as succes, failure, or below confidence threshold. Failures are further categorized as correspondence errors or affordance errors.
           
          </p>
          <div class="training-image">
            <img src="static/images/results_table_web.png" alt="Shirt simulation example"
                style="max-width:100%; height:auto; margin-top:1.5rem; border-radius:8px; margin-bottom: 1rem;">
          </div>

          <h2 class="title is-3" style="text-align:center;">
            Confidence-Based State Machine
          </h2>
          <p>
          This system enables reactive in-air folding by dynamically selecting grasp points based on real-time confidence estimates and recovery failures using tactile reactivity. 
          <strong>A Confidence-Based State Machine </strong> is implemented to guide the folding task by picking the next highest-confidence action. 
          </p>
          <p>
            The system starts by picking the shirt up from the table, looking for correspondence regions above a confidence threshold. At each grasp attempt, the robot can <strong>query from three canonical regions (shoulder, sleeve, bottom) 
            </strong>using the distributional dense correspondence network to generate confidence-weighted heatmaps. 
            A grasp is executed <strong>only if both the correspondence confidence and grasp affordance exceed predefined thresholds.</strong> 
            Otherwise, the robot rotates the garment and reevaluates, ensuring robust grasp point selection across the four folding strategies. 
            
          </p>

            <div style="margin-top: 0rem; margin-bottom: 3rem;">
            <video id="affordance_finetuning" autoplay muted playsinline height="100%"> 
              <source src="./static/videos/confidence_aware_state_machine_web.mp4"
                type="video/mp4">
            </video>
             <p style="font-size:1em; color:#000000;"><i>A confidence-based state machine enables reactive in-air folding by dynamically selecting grasp points based on real-time confidence estimates and recovery failures using tactile reactivity. </i></p>
          </div>
      <h2 class="subtitle has-text-left">
      </div>
      
      <div style="margin-top: 3rem;"></div>


      <h2 class="title is-4">Tensioning</h2>
      <p class="has-text-left">
      Once the shirt is grasped by two keypoints, the robot <strong>tensions</strong> the shirt by <strong>detecting shear via the average marker displacement</strong> on the tactile sensors. 
      Below we show examples of the shirt tensioning step on two different types of shirts.  
      </p>
      <div style="margin-bottom: 1rem;"></div>
      <div class="columns is-centered">
        <!-- Red shortsleeve video -->
        <div class="column is-8">
          <video autoplay controls muted playsinline style="width:100%; height:auto; display:block;">
            <source src="./static/videos/tensioning_redshortsleeve_web.mp4" type="video/mp4">
          </video>
        </div>

        <!-- Orange longsleeve video -->
        <div class="column is-8">
          <video autoplay controls muted playsinline style="width:100%; height:auto; display:block;">
            <source src="./static/videos/tensioning_orangelongsleeve_web.mp4" type="video/mp4">
          </video>
        </div>
      </div>


    <div style="margin-top: 3rem;"></div>
    <!--Folding -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Folding Shirts</h2>
        <div class="content has-text-justified">
         
         
          <div style="margin-top: 1rem;"></div>
          <p>
          Our confidence-aware state machine was able to <strong>successfully</strong> fold the shirts for <strong>6 out of the 10 trials</strong>.
          Of the 30 total grasps attempted during the course of the 10 folding trials, 
          6 were empty grasps successfully caught by the tactile classifier, immediately triggering recovery behaviors, 
          further demonstrating the usefulness of tactile feedback in the folding pipeline. 
          </p>
          <p>
            Of the 4 unsuccessful folding trials, <strong>irrecoverable failure modes included correspondence failures (1 trial), grabbing too much fabric (1 trial), and grabbing diagonally across the shirt (2 trials). </strong>
            (See Appendix 7.2 for for visualizations of these failure modes.) 
          </p>
          <p>
            Furthermore, <strong>without affordance fine-tuning, the folding success rate dropped to 3 out of 10 trials </strong>, with an increase in cases of grabbing too much fabric caused by poor affordance, rather than poor correspondence. 
          </p>
          
          <div style="margin-top: 1rem; margin-bottom: 0 rem; ">
            <video id="affordance_finetuning" autoplay muted loop playsinline controls height="100%"> 
              <source src="./static/videos/example_single_folding_web.mp4"
                type="video/mp4">
            </video>
            <p style="font-size:1em; color:#000000;"><i>Example of a successful folding trial, showing the system step through the 
              confidence-based state machine using both the correspondence and affordance networks  </i></p>
          </div>
          <div style="margin-top: 1rem;"></div>
          <p>
          </p>
          <div style="margin-top: 3rem;"></div>
            <video id="affordance_finetuning" autoplay muted loop playsinline controls height="100%"> 
              <source src="./static/videos/folding_multi_grid_web.mp4"
                type="video/mp4">
            </video>
            <p style="font-size:1em; color:#000000;"><i>Compilation of other successful folding demos, showing a variety of shirt geometries (including 
              longsleeve shirts), materials thicknesses, and dynamics. Different folding strategies, as well as grasps being re-attemped, and the tactile 
              classifier informing the robot to try again are also demonstrated. 
            </i></p>
      <h2 class="subtitle has-text-left">
      </div>
    </div>
    </div>
    <!--/ Folding -->


    <div style="margin-top: 2rem;"></div>
    <!--Hanging -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Hanging Shirts</h2>
        <div class="content has-text-justified">
          <div style="margin-top: 1rem;"></div>
          <p> The task-agnostic grasp selection module can be applied to hanging tasks as well. 
            To perform the hanging task, <strong>collar or shoulder points</strong> are queried from the table and in the air. 
            After securing both grasps, the robot moves open-loop to a peg.
          </p>
          <p>
            Our hanging system was successful in <strong>7 out of 10 trials</strong>, where success is evaluated based on grasping the correct regions and whether the cloth stays on the peg.
            <bold>All 3 failures were attributed to correspondence errors.</bold>
            Example successful hanging videos are shown below. 
          </p>
          
      

      <div style="margin-bottom: 0rem;"></div>
      <div class="columns is-centered">
        <!-- Red shortsleeve video -->
        <div class="column is-8">
          <video autoplay controls muted playsinline style="width:100%; height:auto; display:block;">
            <source src="./static/videos/longsleeve_white_sweater_hanging_web.mp4" type="video/mp4">
          </video>
        </div>

        <!-- Orange longsleeve video -->
        <div class="column is-8">
          <video autoplay controls muted playsinline style="width:100%; height:auto; display:block;">
            <source src="./static/videos/green_sweater_hanging_web.mp4" type="video/mp4">
          </video>
        </div>
      </div>


      <h2 class="subtitle has-text-left">
      </div>
    </div>
    </div>
    <!--/ Hanging -->

  
      



    </div>
    </div>
    <!--/ Combined System -->



<!-- <h3>References - MHT Fix the Formatting Later</h3>
<ol>
  <li id="ref1"> 
    <em>Visuotactile Affordances for Cloth Manipulation with Local Control</em>. 
    <a href="https://arxiv.org/abs/2212.05108" target="_blank" rel="noopener">
      arXiv:1234.5678
    </a>.
  </li>
</ol> -->
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{sunil2025reactiveinairclothingmanipulation,
      title={Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance}, 
      author={Neha Sunil and Megha Tippur and Arnau Saumell and Edward Adelson and Alberto Rodriguez},
      year={2025},
      url={https://arxiv.org/abs/2509.03889}, 
}
</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="static/pdfs/CoRL2025_InAirReactiveManipulation.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://mhtippur.github.io/inairclothmanipulation/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website template is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
