<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A visuotactile system capable of folding and hanging in-air using dense visual representations and tactilely-supervised visual affordance networks.">
  <meta name="keywords" content="Deformable Object Manipulation, Dense Correspondence Learning, Confidence-Aware Planning, Visuotactile Perception">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/shirticon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
<!--
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>
-->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="http://nehasunil.com/" target="_blank">Neha Sunil</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://mhtippur.github.io/web/" target="_blank">Megha Tippur</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a> Arnau Portillo</a>,
                    <span class="author-block">
                    <a href="https://persci.mit.edu/people/adelson/" target="_blank"> Edward Adelson</a>,
                    <span class="author-block">
                    <a href="https://mcube.mit.edu/index.html" target="_blank"> Alberto Rodriguez</a>
                  </span>
          </div>

                <div class="is-size-5 publication-authors">
                  <img src="static/images/mit_logo_red.png" alt="MIT Logo" style="height: 2.5em; vertical-align: middle;"><br>
                  CoRL 2025 <i>(Oral Presentation)</i></span>
                  <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="static/pdfs/CoRL2025_InAirReactiveManipulation.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Supplementary Link-->
              <span class="link-block">
                <a href="static/pdfs/CoRL2025_Supplemental_InAirReactiveManipulation.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://mhtippur.github.io/inairclothmanipulation/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. 
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. 
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. 
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser_video.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-left">
        <strong>Overview:</strong> We develop a visuotactile system capable of folding and hanging in-air using 
        dense visual representations and tactilely-supervised visual affordance networks.
      </h2>
    </div>
  </div>
</section>

<section class="section pt-3">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div style="background-color: #f5f5f5; padding: 1.5rem; border-radius: 6px;">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Manipulating clothing is challenging due to complex configurations, variable material dynamics, and frequent self-occlusion. 
          Prior systems often flatten garments or assume visibility of key features.
          We present a dual-arm visuotactile framework that combines confidence-aware dense visual correspondence and tactile-supervised grasp affordance to operate directly on crumpled and suspended garments. 
          </p>
          <p>
          The correspondence model is trained on a custom, high-fidelity simulated dataset using a distributional loss that captures cloth symmetries and generates correspondence confidence estimates. 
          These estimates guide a reactive state machine that adapts folding strategies based on perceptual uncertainty. 
          In parallel, a visuotactile grasp affordance network, self-supervised using high-resolution tactile feedback, determines which regions are physically graspable. 
          The same tactile classifier is used during execution for real-time grasp validation. 
          By deferring action in low-confidence states, the system handles highly occluded table-top and in-air configurations. 
          We demonstrate our task-agnostic grasp selection module in folding and hanging tasks. 
          Moreover, our dense descriptors provide a reusable intermediate representation for other planning modalities, such as extracting grasp targets from human video demonstrations, paving the way for more generalizable and scalable garment manipulation.
          </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <div style="margin-top: 2rem;"></div>

    <!-- Simulation -->
     <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Dataset Generation in Simulation</h2>
        <div class="content has-text-justified">
          <!-- <p>
            We use <strong>Blender</strong> to simulate a wide variety of shirt geometries and deformations, generating a large RGB-D dataset (1500 scenes) for training. 
            In addition to parameterizing the overall geometries, we incorporate <strong>hems, stitches, and sewing seams</strong> into our shirts to mimic realistic garments, enhancing visual realism 
            and providing key features helpful for correspondence. 
            Our method incorporates these finer details while <strong>preserving consistent vertex indexing across shirts</strong>, 
            enabling descriptors to align with a canonical template regardless of geometry, without relying on sparse skeleton keypoints.
          </p> -->
          <p>
            We use <strong>Blender</strong> to simulate a wide variety of shirt geometries and deformations. We incorporate <strong>hems, stitches, and sewing seams </strong> 
            into our simulations to mimic realistic garments, enhaving visual realism and providing key features to correspondence. We use <strong>consistent vertex 
              indexing across shirts </strong>, enabling descriptors to align with a canonical template. 
          </p>
          
          <!-- Simulation Image -->
      <div class="simulation-image">
        <img src="static/images/blender_simulation_methodology.png" alt="Shirt simulation example"
            style="max-width:100%; height:auto; margin-top:1.5rem; border-radius:8px; margin-bottom: 1.5rem;">

          <p>
            Our animated simulation pipeline enables generation of a wide range of shirt types and configurations. Shown below are a few examples 
            different simulated shirts in both hanging in-air and on-table configurations. 
          </p>
          
          <div class="example-shirts-image">
            <img src="static/images/example_simulated_shirts.png" alt="Shirt simulation example"
                style="max-width:100%; height:auto; margin-top:1.5rem; border-radius:8px;">
        </div>
      </div>
    </div>
    <!--/ Simulation -->

    <div style="margin-top: 5rem;"></div>
    <!--Dense Correspondence with Distributive Loss -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Dense Correspondence with Distributional Loss</h2>
        <div class="content has-text-justified">

          <!-- Network Training Architecture -->

          <p>
            To train the dense correspondence network, a pixel on the deformed shirt is queried. Because the vertex scheme across the different simulated shirts is consistent with 
            the canonical shirt mapping, the corresponding pixel match in simulation is found. The network outputs a dense descriptor for the shirt 
            where every pixel is mapped to an d-dimensional embedding, capturing the location of the pixel with respect to the canonical shirt in the descriptor space. 
          </p>
          <div class="training-image">
            <img src="static/images/training_new.png" alt="Shirt simulation example"
                style="max-width:100%; height:auto; margin-top:1.5rem; border-radius:8px;">
          </div>
          <div style="margin-top: 2rem;"></div>

          <!-- <p>
            <strong>MHT MAYBE TAKE THIS PARAGRAPH OUT - TOO IN DEPTH FOR WEBSITE! </strong>Additionally, we train the network using a distributional loss. A multimodal isotropic Gaussian with modes at symmetric match points is used as 
            the target distribution. The probability estimator is found by taking the softmax over negative distances in the descriptor space. The model is 
            trained by minimizing the difference between the target distribution and the probability estimate, as measured by KL divergence. Training with 
            a distributional loss is advantageous because 
          </p> -->
          <p>
            Using a <strong>Distributional Loss </strong> for training our dense correspondence network provides several advantages, including allowing the network 
            to represent multiple valid matches for a pixel, <strong>capturing symmetries in the cloth</strong>, and by providing a <strong>built-in confidence metric</strong> needed to manipulate 
            garments in highly occluded states.  
          </p>
          <p>
            The resulting network can query points on deformed shirts and get correspondence heatmaps with respect to the canonical shirt. 
            We can also use our descriptors in the <strong>inverse direction</strong> to query points on the canonical shirt and find correspondences on the deformed shirt. 
            <strong>This is the direction we query in during execution on the robot.</strong>
          </p>
          <div class="training-image" style="margin-top: 0rem; margin-bottom: 0rem;">
            <img src="static/images/dense_correspondence.png" alt="Shirt simulation example"
                style="max-width:100%; height:auto; margin-top:1.5rem; border-radius:8px;">
          </div>
        

      </div>
    </div>
    <!--/ Dense Correspondence with Distributive Loss -->
  </div>

  <div style="margin-top: 2rem;"></div>
    <!--Visuotactile Grasp Affordance -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Visuotactile Grasp Affordance Network</h2>
        <div class="content has-text-justified">
          <p>
          We train a <strong>grasp affordance network to find good grasp regions of a hanging shirt </strong> using the simulated dataset. A depth image of the garment 
          is input into the UNet archiecture implemented in <a href="#ref1">[1]</a> and a heatmap representing the graspable regions is output. 
          </p>
          <p>By training on the simulated shirt dataset, we can explicitly check desired grasping criteria, such as <strong> (1) side grasp reachability, 
            (2) gripper collision, and (3) whether or not two layers of fabric or less are being grasped </strong>
          </strong></p>
          <div class="training-image">
            <img src="static/images/affordance_training_in_simulation.png" alt="Shirt simulation example"
                style="max-width:100%; height:auto; margin-top:1.5rem; border-radius:8px;">
          </div>
          <div style="margin-top: 1rem;"></div>
          <p> However, because there are a wide range of material properties and dynamics from varied fabrics that cannot be fully captured in simulation, 
            we <strong>fine-tune on the robot using a tactile classifier to supervise grasp success</strong>. The visualized grasp affordance 
            shows predictions from the model trained in simulation, and the selected grasps based on that affordance to improve efficiency of grasp selection. 
          </p>
          <div style="margin-top: 3rem;"></div>
            <video id="affordance_finetuning" autoplay muted loop playsinline controls height="100%"> 
              <source src="./static/videos/finetuning_grasp_affordance_website.mp4"
                type="video/mp4">
            </video>
      <h2 class="subtitle has-text-left">
      </div>
    </div>
    </div>
    <!--/ Visuotactile Grasp Affordance -->

    <div style="margin-top: 3rem;"></div>
    <!--Combined System -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Combined System</h2>
        <div class="content has-text-justified">
          
          <div style="margin-top: 0rem; margin-bottom: 3rem; text-align: center;">
            <video id="affordance_finetuning" autoplay muted loop playsinline controls height="100%"> 
              <source src="./static/videos/single_affordance_grasp_video_web.mp4"
                type="video/mp4">
            </video>
             <p style="font-size:1em; color:#000000;">Example of the combined system chosing the best correspondence match according to the network.</p>
          </div>

          <p>
          In order to go through the multiple steps for folding, we implement a <strong>Confidence-Aware State Machine</strong> to find the next grasps.
          </p>

            <div style="margin-top: 0rem; margin-bottom: 3rem; text-align: center;">
            <video id="affordance_finetuning" autoplay muted playsinline height="100%"> 
              <source src="./static/videos/confidence_aware_state_machine_web.mp4"
                type="video/mp4">
            </video>
             <p style="font-size:1em; color:#000000;">Confidence-Aware State Machine</p>
          </div>
      <h2 class="subtitle has-text-left">
      </div>
      
      <div style="margin-top: 3rem;"></div>

      <p class="has-text-left"></p>

      <div style="margin-bottom: 3rem;"></div>
      <div class="columns is-centered">
        <!-- Red shortsleeve video -->
        <div class="column is-8">
          <video autoplay controls muted playsinline style="width:100%; height:auto; display:block;">
            <source src="./static/videos/tensioning_redshortsleeve_web.mp4" type="video/mp4">
          </video>
        </div>

        <!-- Orange longsleeve video -->
        <div class="column is-8">
          <video autoplay controls muted playsinline style="width:100%; height:auto; display:block;">
            <source src="./static/videos/tensioning_orangelongsleeve_web.mp4" type="video/mp4">
          </video>
        </div>
      </div>


    <div style="margin-top: 2rem;"></div>
    <!--Folding -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Folding Shirts</h2>
        <div class="content has-text-justified">
         
         
          <div style="margin-top: 1rem;"></div>
          <p>
          </p>
          <div style="margin-top: 3rem;"></div>
            <video id="affordance_finetuning" autoplay muted loop playsinline controls height="100%"> 
              <source src="./static/videos/example_single_folding_web.mp4"
                type="video/mp4">
            </video>

          <div style="margin-top: 1rem;"></div>
          <p>
          </p>
          <div style="margin-top: 3rem;"></div>
            <video id="affordance_finetuning" autoplay muted loop playsinline controls height="100%"> 
              <source src="./static/videos/folding_multi_grid_web.mp4"
                type="video/mp4">
            </video>
      <h2 class="subtitle has-text-left">
      </div>
    </div>
    </div>
    <!--/ Folding -->


        <div style="margin-top: 2rem;"></div>
    <!--Hanging -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Hanging Shirts</h2>
        <div class="content has-text-justified">
         
         
          <div style="margin-top: 1rem;"></div>
          <p> </p>
          
      

      <div style="margin-bottom: 3rem;"></div>
      <div class="columns is-centered">
        <!-- Red shortsleeve video -->
        <div class="column is-8">
          <video autoplay controls muted playsinline style="width:100%; height:auto; display:block;">
            <source src="./static/videos/longsleeve_white_sweater_hanging_web.mp4" type="video/mp4">
          </video>
        </div>

        <!-- Orange longsleeve video -->
        <div class="column is-8">
          <video autoplay controls muted playsinline style="width:100%; height:auto; display:block;">
            <source src="./static/videos/green_sweater_hanging_web.mp4" type="video/mp4">
          </video>
        </div>
      </div>


      <h2 class="subtitle has-text-left">
      </div>
    </div>
    </div>
    <!--/ Hanging -->

  
      



    </div>
    </div>
    <!--/ Combined System -->



<!-- <h3>References - MHT Fix the Formatting Later</h3>
<ol>
  <li id="ref1"> 
    <em>Visuotactile Affordances for Cloth Manipulation with Local Control</em>. 
    <a href="https://arxiv.org/abs/2212.05108" target="_blank" rel="noopener">
      arXiv:1234.5678
    </a>.
  </li>
</ol> -->
</section>






<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="static/pdfs/CoRL2025_InAirReactiveManipulation.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://mhtippur.github.io/inairclothmanipulation/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website template is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
